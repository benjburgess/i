---
title: "Support Vector Machines"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
---

[benjburgess.github.io](https://benjburgess.github.io/)

[Support Vector Machines](https://benjburgess.github.io/data/index/svm)

# Introductory Tutorial

## Introduction

As I've detailed in the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm), SVMs are a machine learning tool which can be used to classify data into two or more different groups. Outlined below is an example of how an SVM approach can be applied to a relatively complex dataset, in this case considering the properties of over 6000 different wines. Here, we will build a SVM capable of accurately classifying the quality of wine based on its chemical properties. Here, quality is a category between 1 and 10, with one reflecting the lowest quality wine and 10 the highest quality wine. For this example, we will be using `R`.

The first thing to do is download the data and save it to your local machine. The data is available from [Kaggle](https://www.kaggle.com/shelvigarg/wine-quality-dataset) and is from an academic paper by [Cortez et al. (2009)](https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377).

Throughout this tutorial the majority of the concepts I refer to are explained in more detail in the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm). Wherever this is not the case, I've added in references and links to other sources. Throughout this tutorial, code chunks will have a blue background, output will have a green background, and unsuccessful output will have a red background.

## Packages and set.seed()

Firstly, we need to install the various packages we need for this tutorial. If any of these packages are not installed in your local machine, they can be installed via the `install.packages()` function. We will also set the seed for this code, this is simply to allow reproducibility of the results.

```{r class.source = "bg-info", message=FALSE, warning=FALSE}
library(e1071)
library(ggplot2)
library(data.table)
library(scales)
library(scutr)
library(caret)

set.seed(754)
```

## Data loading

The next thing we need to do is load in the data. This will be from wherever you saved the data on your local machine and whatever you titled the file. 

At this point we will also change some of the column names. In the original file there were blank spaces which `R` isn't a fan of. The following code will also convert any of these blank spaces to underscores.

```{r class.source="bg-info", class.output="bg-success"}
df <- fread("~/R_sandbox/Wine_Data_Kaggle.csv", data.table=FALSE)

colnames(df) <- gsub(" ", "_", colnames(df))

head(df)
```


## Data cleaning

At this point when checking our data it is apparent that there are a few rows with missing data. For our analysis we will remove these rows as they will convolute any analysis.

```{r class.source="bg-info", class.output="bg-success"}
original.dim <- dim(df)[1]

df <- na.omit(df)

new.dim <- dim(df)[1]

original.dim - new.dim

```

As you can see there are `r original.dim - new.dim` rows which have been removed from our dataframe.

The next step is to convert the categorical variable of `type` into a binary variable. At present the column of `type` is either 'white' or 'red' however the SVM doesn't really respond to data in this form. We are much better off encoding this information as two dummy variables, namely `white` and `red`. Wherever `type` is 'white', `white` will have a value of 1 and `red` a value of 0. Wherever `type` is 'red', `white` will have a value of 0 and `red` a value of 1. In effect we have encoded the information in the `type` column in a slightly different manner. See [this link on stackoverflow](https://stats.stackexchange.com/questions/52915/how-to-deal-with-an-svm-with-categorical-attributes) for a more thorough explanation of why this is necessary. However, once this is done we can then remove the original `type` column.

```{r class.source="bg-info", class.output="bg-success"}
df$white <- NA
df$red <- NA

red_white_binary <- function(wine, wine_type){
  
  if(wine == wine_type){
    return(1)
  }
  
  if(wine != wine_type){
    return(0)
  }
  
}

df$white <- mapply(red_white_binary, df$type, "white")
df$red <- mapply(red_white_binary, df$type, "red")

#remove 'type' column as this has been replaced by a binary red (0,1) or white (0,1)
df <- df[,-1]
```

If we have a look at our dataset, we can plot a histogram of the different count of wines in each of the different categories. 
```{r class.source="bg-info", class.output="bg-success", warning = FALSE, message = FALSE}
ggplot(df, aes(x=as.numeric(quality))) + 
  geom_histogram(color="black", fill="white") + 
  xlab("quality")
```

As we can see, there are a few instances of wines with quality 3, 4, and 8 although most wines have a quality rating of either 5, 6, or 7. Importantly, there are also five wines which have a quality rating of 9.

This poses an issue for our analysis. Our dataset is heavily imbalanced, as shown by the high frequency of wines with qualities of 5 or 6. As such, we could in theory get a reasonable accuracy for our model if it predicted groups 5 and 6 accurately but all other groups poorly. This would represent a biased model. As such one way we can go about this is using a technique called SMOTE or SCUT to rebalance our dataset to have equal proportions of each of the different wine qualities. SMOTE [(explained more here)](https://towardsdatascience.com/a-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e) is method for generating new data based on existing data in the same group. I'm not going to go into it too much here, but this technique uses the nearest neighbour algorithm to generate new data. SMOTE is the method to use if your data only contains two groups, while SCUT is the method to use if your data comprises three or more groups. Here, we have seven different groups so we will be using SCUT.

SCUT can be used from the `scutr` package.

But first, there's an issue with our dataset. For the SCUT algorithm to work, we have to have at least six individual data points for each class. However, we only have five wines of quality 9. As such, the algorithm isn't going to work. This is a [known issue](https://stackoverflow.com/questions/61772147/tuning-smotes-k-with-a-trafo-fails-warningk-should-be-less-than-sample-size) with the SCUT function, but at the moment we don't have many options for our analysis. 

As such, for our analysis we are going to drop all five wines with quality 9, as otherwise see can't proceed. This isn't an ideal solution, but at present no other package or function in `R` can implement the SCUT algorithm.

As such, our code becomes:

```{r class.source="bg-info", class.output="bg-success"}

df <- subset(df, quality != 9)

#convert quality to a factor from a numeric value
df$quality <- factor(df$quality)

df <- SCUT(df, "quality", oversample="oversample_smote")

```

As such if we replot our dataset, using the following code, we can see that we've now got equal counts of each of the six remaining wine qualities in our dataset.

```{r class.source="bg-info", class.output="bg-success", warning = FALSE, message = FALSE}
ggplot(df, aes(x=as.numeric(as.character(quality)))) + 
  geom_histogram(color="black", fill="white") + 
  xlab("quality")
```

Now if we look at our dataset one thing we can easily notice is that the different variables are all on different scales.

```{r class.source="bg-info", class.output="bg-success"}
head(df)
```

[This is an issue for SVMs](https://stats.stackexchange.com/questions/65094/why-scaling-is-important-for-the-linear-svm-classification/224201), as if data is on different scales there is the potential for the numerically larger variables to be given a greater importance than the numerically smaller variables. This then has the potential to bias our model. As such, we need to rescale each variable to be on the same scale. This can be done with the following code, [see this stackoverflow link for more details](https://stats.stackexchange.com/questions/178626/how-to-normalize-data-between-1-and-1).

```{r class.source="bg-info", class.output="bg-success"}
for(i in 1:(dim(df)[2]-3)){
  
  #This code rescales all variables apart from the final three columns which 
  #are already scaled (red and white) or a factor (quality)
  
  df[,i] <- rescale(df[,i], to=c(-1,1))
}
```

## Data splitting

At this point, we've now finished cleaning our data and can almost start the analysis.

The final thing we need to do is to split our data into a test and training datasets. 
Our analysis is going to be run on the training dataset before being tested on the test dataset right at the very end of the analysis.

For our analysis, the training dataset will comprise 80% of the dataset and the test datset will comprise 20% of the original dataset. Although a 80/20 split is relatively common, other analyses may use a different split (e.g., 67/33) for the training and test datasets.

To split our data we use the following code.
```{r class.source="bg-info", class.output="bg-success"}
row_nums <- sample.int(dim(df)[1], round(0.8*dim(df)[1]))

df.train <- df[row_nums,]

df.test <- df[-row_nums,]
```


## Training the SVM

At this point we are now ready to start training our SVM. However, we have to make several decisions about our model. Firstly, we have to decide which kernel to use. For a refresher on kernel choice, see the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm). 

For this analysis, we will be using a radial kernel. 

The next decision we need to make is what values for the cost and gamma parameters we should specify. For a refresher on these parameters, see the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm). Fortunately, the SVM package we are using (e1071) allows us to test various different parameters and then select the best ones.

For the training and testing of the SVM we will be following the advice of [Hsu et al. (2003)](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf). Hsu et al. (2003) advise that SVMs should be generated in two distinct sections. Firstly, a relatively coarse selection of parameter values should be selected which are used to train the model against the training dataset (i.e., training the model). At this point the best parameter values are identified. These parameter values are then used in the next stage (i.e., tuning the model). For tuning the model, a relatively narrow selection of parameter values are then selected which are based on the best parameter values from the model training. This allows for potentially even better model parameters to be identified. 

The range of parameter values to be used is usually at the discretion of the user, although it is usually on a logarithmic scale. Hsu et al. (2003) suggest that values for cost could be $2^{-5}$, $2^{-3}$, ... $2^{15}$ while for gamma the values could be $2^{-15}$, $2^{-13}$, ... $2^{3}$. In the interests of speed, for this analysis we will be using cost parameter values of $2^{-2}$, $2^{-1}$, ... $2^{6}$ and gamma parameter values of $2^{-6}$, $2^{-5}$, ... $2^{2}$.

When training the SVM, the best model parameters are chosen by those which generate an SVM which minimises wines being assigned to the incorrect class (i.e., the model which correctly assigns most wines to the correct class). As part of this procedure, we will be using k-fold cross validation as way of increasing the robustness of our results. I'm not going to go into k-fold cross validation here, but for more information see [this explanation](https://machinelearningmastery.com/k-fold-cross-validation/).

Following this, our SVM can be trained using the following code.

Note that e1071 package conducts its analysis consecutively, as opposed to parallelly, and can take a while to run (potentially over an hour).

```{r class.source="bg-info", class.output="bg-success"}
svm_trained  <- tune.svm(quality ~ ., #determine wine quality from all other variables
                       data=df.train, #using the df.train dataset
                       tunecontrol = tune.control(cross=10), # use k-fold cross validation with k=10
                       gamma=2^c(seq(-6, 2, by=1)), #select our range of gamma values 
                       cost=2^c(seq(-2, 6, by=1))) #select our range of cost values


summary(svm_trained) #show the summary of our svm training
```

So, as we can see from the model training, the best performing model was when gamma equals `r as.numeric(svm_trained$best.parameters[1])` and cost equals `r as.numeric(svm_trained$best.parameters[2])`. Here the best performance of the model was `r as.numeric(svm_trained$best.performance)` which means that across all 10 folds of the cross validation for these parameter values, ~`r 100*(1-as.numeric(svm_trained$best.performance))`% of all wines were assigned the correct category. This isn't too bad, but there is the potential to further increase this by tuning our SVM. 

However, first we need to extract the best performing model using the following code.


```{r class.source="bg-info", class.output="bg-success"}
svm_trained_model <- svm_trained$best.model
```


## Tuning the SVM

At this stage we have trained the svm and identified the initial parameter values for gamma and cost. In keeping with the recommendations of [Hsu et al. (2003)](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) the next stage of the model development is to tune the model. At this stage, rather than use the coarse gradient of values for each parameter, we use a much more narrow gradient which should allow us to identify better parameter values for our SVM. At this stage for the cost and gamma parameters, we now consider values in the following range which is centerd on the best model parameters from the training stage of the model development: $2^{n - 0.75}$, $2^{n - 0.5}$, $2^{n - 0.25}$, $2^{n}$,... $2^{n+0.75}$. 

The model tuning can be run using the following code.
```{r class.source="bg-info", class.output="bg-success"}
svm_tuned  <- tune.svm(quality ~ ., 
                       data=df.train, 
                       tunecontrol = tune.control(cross=10),
                       gamma=2^c(seq(log(svm_trained_model$gamma, base=2)-0.75, 
                                     log(svm_trained_model$gamma, base=2)+0.75, by=1/4)),
                        cost=2^c(seq(log(svm_trained_model$cost, base=2)-0.75, 
                                     log(svm_trained_model$cost, base=2)+0.75, by=1/4)))

summary(svm_tuned)
```

So, as we can see from our tuning of the SVM, the best performing model has now changed to where gamma equals `r as.numeric(svm_tuned$best.parameters[1])` and cost equals `r as.numeric(svm_tuned$best.parameters[2])`. Here the best performance of the model has improved to `r as.numeric(svm_tuned$best.performance)` which means that across all 10 folds of the cross validation for these parameter values, ~`r 100*(1-as.numeric(svm_tuned$best.performance))`% of all wines were assigned the correct category. At this stage we'll move from tuning our SVM through to the testing stage.

As before, we need to extract the best performing model using the following code.

```{r class.source="bg-info", class.output="bg-success"}
final_svm_model <- svm_tuned$best.model
```


## Testing the SVM

So as we discussed above, the best forming model is where gamma equals `r as.numeric(svm_tuned$best.parameters[1])` and cost equals `r as.numeric(svm_tuned$best.parameters[2])`. Here across all 10 folds of the cross validation ~`r 100*(1-as.numeric(svm_tuned$best.performance))`% of all wines were assigned to the correct category. This can be seen using the following code.

```{r class.source="bg-info", class.output="bg-success"}
svm_tuned
```

This analysis has so far been based on the training dataset, but at this stage we can now shift our attention to the test dataset we subset from the original data. Using our SVM we can now predict the quality of every wine in the test dataset and compare our predictions to the actual class of the wine. From this we can calculate various different metrics which we can use to assess the performance of our SVM. This can be done using the following code.


```{r class.source="bg-info", class.output="bg-success"}
pred=predict(final_svm_model, df.test)

cm <- caret::confusionMatrix(pred, df.test$quality) 
#this uses the confusionMatrix function from the caret package
cm

```

There's an awful lot of information here, so let's breakdown piece by piece to assess our SVM.

Firstly, let's start with the confusion matrix using the following code.

```{r class.source="bg-info", class.output="bg-success"}

cm$table

```

The confusion matrix is basically a way of allowing us to visually assess whether our SVM assigns our wines (from the test dataset) the correct qualities or, if not, which qualities they are assigned. From our dataset we can see that the SVM broadly classifies wine of qualities 3, 4, or 8 correctly. However, the SVM appears to be less good at classifying wines of qualities 5, 6, and 7. This is just a visual inspection and our initial thoughts can be confirmed (or contrasted) using some statistics.

Let's start by considering the overall statistics for our model, which can be called using the following code.

```{r class.source="bg-info", class.output="bg-success"}

cm$overall

```

So, of the metrics here, the main ones we are going to consider are accuracy and kappa.

If we start with accuracy, the accuracy of our SVM for the test dataset is `r as.numeric(cm$overall[1])*100`% which is pretty similar to the accuracy we obtained from the k-fold cross validation of the SVM at the tuning stage (~`r 100*(1-as.numeric(svm_tuned$best.performance))`%). Overall, our model does a fairly good job of classifying the quality of wines. However, in our test dataset, `r 100 - as.numeric(cm$overall[1])*100`% of wines were incorrectly classified which is higher than we might like. 

The next metric to consider is the kappa value. The kappa value is an alternative measure of accuracy which considers the expected and observed accuracy of our SVM. I'm not going to explain kappa in any more detail (but see [this comprehensive explanation on stackoverflow](https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english)), but as with the accuracy metric the higher the value of kappa the better. As we can see, the measure of kappa for our analysis is `r as.numeric(cm$overall[2])`. This value of kappa indicates that our SVM actually does a `r if(as.numeric(cm$overall[2]) >= as.numeric(cm$overall[1])){"slightly better"}``r if(as.numeric(cm$overall[2]) < as.numeric(cm$overall[1])){"slightly worse"}` job of correctly classifying wine quality than is suggested by the SVM accuracy. Overall, these metrics are two of the most commonly used metrics for assessing the performance of an SVM. 

The final thing we can do when assessing our SVM performance is to consider how the performance varies between different wine qualities (much as we visually did for the confusion matrix above). This can be done by running the following code. 

```{r class.source="bg-info", class.output="bg-success"}

cm$byClass

```

As we can see we have a variety of different metrics for each of the different wine qualities. Here, we are only going to focus on the final column of balanced accuracy, but [an explanation of the other metrics can be found here](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124?gi=f0ce52bb598).

We can cut out the other metrics, and focus on balanced accuracy using the following code.

```{r class.source="bg-info", class.output="bg-success"}

cm$byClass[,11]*100

```

While the variable name is now missing, by comparing to the above results we can confirm that this is indeed balanced accuracy which has simply been converted to a percentage.

As you can see, for wines of qualities 3, 4, and 8 the balanced accuracy is as high as `r max(as.numeric(cm$byClass[,11]*100))`%. Much as we suspected from our inspection of the confusion matrix, our SVM is really very good at classifying wines of either a very high or very poor quality. However, we can also see that for wines of qualities 5, 6, and 7, the balanced accuracy is as low as `r min(as.numeric(cm$byClass[,11]*100))`%. This suggests that while our model still does a reasonable job of classifying wines of these qualities, it is not as good as we would like. This is really important to understand as while our overall model accuracy is `r as.numeric(cm$overall[1])*100`% this actually varies between the different wine qualities. This might be really important for our analyses, as if our aim was to build a machine learning model which is capable of accurately determining when a wine is of a very poor or very high quality then the model does an excellent job of this. However, if our aim was to build a model which is capable of accurately determining wines of medium quality (i.e., qualities 5, 6, and 7) then we may need to rethink our analysis as our model is less good at doing this.

## Improving the SVM

As we've shown from the model testing we have developed an SVm which does a good job of classifying wines into different qualities based on their chemical properties. However, we may wish to further improve the model. As detailed above, the main improvement we might wish to make is to increase the abilit of the SVM to accurately classify wines of qualities 5, 6, and 7. We may be able to achieve this by further tuning the SVM with an even narrower range of parameter values for cost and gamma, however we need to ensure that we are not going to overfit the model when doing this. Alternatively if we are only concerned about identifying wines of qualities 5, 6, and 7 then we might wish to rerun the entire analysis but solely focussing on these qualities. This will likely mean that any SVM will not be able to determine if a wine is of qualities 3, 4, or 8, but may instead accurately distinguish between the other qualities. While we have likewise encoded wine type within our dataset and analysis, we could always in theory split the dataset in two to only consider white or red wines. Overall, improving the model is likely a trade-off between many different choices. 



## Concluding remarks

In this tutorial, we have build a support vector machine (SVM) which does a good job of classifying wines of different qualities. In doing so, I've outlined the different data cleaning and analysis stages which are essential for the generation of an effective SVM. However, we've also identified where our SVM excels and where it could potentially be improved. While this SVM has been tuned, it is possible that with further tuning the SVM could be improved even further. Overall, this is simply an introductory tutorial to what is a complex approach. Finally, for those interested, the accuracy of our approach and SVM can be contrasted with that of the original study of this dataset by [Cortez et al. (2009)](https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377).



## References

Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Modeling wine preferences by data mining from physicochemical properties. *Decision Support Systems*, 47(4), 547-553.

Hsu, C. W., Chang, C. C., & Lin, C. J. (2003). *A practical guide to support vector classification*.


## Links

[benjburgess.github.io](https://benjburgess.github.io/)

[Support Vector Machines](https://benjburgess.github.io/data/index/svm)

