---
title: "Support Vector Machines"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
---

[benjburgess.github.io](https://benjburgess.github.io/)

[Support Vector Machines](https://benjburgess.github.io/data/index/svm)

# Advanced Tutorial

## Introduction

As I've detailed in the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm), SVMs are a machine learning tool which can be used to classify data into two or more different groups. Outlined below is an example of how an SVM approach can be applied to a relatively complex dataset, in this case considering the properties of over 6000 different wines. Here, we will build a SVM capable of accurately classifying the quality of wine based on its chemical properties. Here, quality is a category between 1 and 10, with one reflecting the lowest quality wine and 10 the highest quality wine. For this example, we will be using `R`.

The first thing to do is download the data and save it to your local machine. The data is available from [Kaggle](https://www.kaggle.com/shelvigarg/wine-quality-dataset) and is from an academic paper by [Cortez et al. (2009)](https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377).

Throughout this tutorial the majority of the concepts I refer to are explained in more detail in the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm). Wherever this is not the case, I've added in references and links to other sources. Throughout this tutorial, code chunks will have a blue background and output will have a green background.

This tutorial is meant to be a relatively advanced example of SVMs, particularly with regards to the number of different classes (i.e., wine qualities) and data cleaning. For a simpler workflow of SVMs in R, see the [Introductory Tutorial on SVMs](https://benjburgess.github.io/i/SVM_linear).


## Packages and set.seed()

Firstly, we need to install the various packages we need for this tutorial. If any of these packages are not installed in your local machine, they can be installed via the `install.packages()` function. We will also set the seed for this code, this is simply to allow reproducibility of the results.

```{r class.source = "bg-info", message=FALSE, warning=FALSE}
library(e1071)
library(ggplot2)
library(data.table)
library(scales)
library(scutr)
library(caret)
library(caTools)

set.seed(7654)
```

## Data loading

The next thing we need to do is load in the data. This will be from wherever you saved the data on your local machine and whatever you titled the file. 

At this point we will also change some of the column names. In the original file there were blank spaces which `R` isn't a fan of. The following code will also convert any of these blank spaces to underscores.

```{r class.source="bg-info", class.output="bg-success"}
df <- fread("~/R_sandbox/Wine_Data_Kaggle.csv", data.table=FALSE)

colnames(df) <- gsub(" ", "_", colnames(df))

head(df)
```


## Data cleaning

At this point when checking our data it is apparent that there are a few rows with missing data. For our analysis we will remove these rows as they will convolute any analysis.

```{r class.source="bg-info", class.output="bg-success"}
original.dim <- dim(df)[1]

df <- na.omit(df)

new.dim <- dim(df)[1]

original.dim - new.dim

```

As you can see there are `r original.dim - new.dim` rows which have been removed from our dataframe.

The next step is to convert the categorical variable of `type` into a binary variable. At present the column of `type` is either 'white' or 'red' however the SVM doesn't really respond to data in this form. We are much better off encoding this information as two dummy variables, namely `white` and `red`. Wherever `type` is 'white', `white` will have a value of 1 and `red` a value of 0. Wherever `type` is 'red', `white` will have a value of 0 and `red` a value of 1. In effect we have encoded the information in the `type` column in a slightly different manner. See [this link on stackoverflow](https://stats.stackexchange.com/questions/52915/how-to-deal-with-an-svm-with-categorical-attributes) for a more thorough explanation of why this is necessary. However, once this is done we can then remove the original `type` column.

```{r class.source="bg-info", class.output="bg-success"}

#generate the new columns
df$white <- NA
df$red <- NA

#a function to populate the values of the dummy variables
red_white_binary <- function(wine, wine_type){
  
  if(wine == wine_type){
    return(1)
  }
  
  if(wine != wine_type){
    return(0)
  }
  
}

#generate the values for white and red columns
df$white <- mapply(red_white_binary, df$type, "white")
df$red <- mapply(red_white_binary, df$type, "red")

#remove 'type' column as this has been replaced by a binary red (0,1) or white (0,1)
df <- df[,-1]
```

At this point we need to split our data into two distinct subsets. The first will be used for training the svm (the train dataset) and the second will be used for testing the dataset.

First of all, we're going to remove wines with a quality 9 from our dataset. We've going to do this for two reasons. Firstly, we only have five wines of quality 9 in our dataset, this means it is unlikely that our SVM will be able to classify these wines with any great accuracy given the limited data. Secondly, for the SCUT algorithm to work (see below) we need at least six data points for each wine quality in the train dataset. As we only have five wines overall, the SCUT algorithm won't work. This isn't an ideal situation; however, for our analysis, this is the best solution at this point. 

```{r class.source="bg-info", class.output="bg-success"}

df <- subset(df, quality  != 9)

df$quality <- factor(df$quality)

```

At this stage we can now split our data. Here, we will be splitting our data preserving the same ratio of wines of quality 3 - 8 in both the test and train datasets. 

Our analysis is going to be run on the training dataset before being tested on the test dataset right at the very end of the analysis.

For our analysis, the training dataset will comprise 70% of the dataset and the test datset will comprise 30% of the original dataset. Although a 70/30 split is relatively common, other analyses may use a different split (e.g., 67/33) for the training and test datasets.

This can be done using the following code.

```{r class.source="bg-info", class.output="bg-success"}

split_determined  <- sample.split(df$quality, SplitRatio = 0.7)
df.train <- df[split_determined,]
df.test  <- df[!split_determined,]

```


Now if we look at our train and test dataset one thing we can easily notice is that the different variables are all on different scales.

```{r class.source="bg-info", class.output="bg-success"}
head(df.train)
```

[This is an issue for SVMs](https://stats.stackexchange.com/questions/65094/why-scaling-is-important-for-the-linear-svm-classification/224201), as if data is on different scales there is the potential for the numerically larger variables to be given a greater importance than the numerically smaller variables. This then has the potential to bias our model. As such, we need to rescale each variable to be on the same scale. This can be done with the following code, [see this stackoverflow link for more details](https://datascience.stackexchange.com/questions/13971/standardization-normalization-test-data-in-r).


```{r class.source="bg-info", class.output="bg-success"}

train_scaling <- preProcess(df.train, method=c("scale", "center"))
df.train <- predict(train_scaling, df.train)
df.test  <- predict(train_scaling, df.test)

```

This code ensures that the test and train datasets are both scaled in the same way. If they weren't when we come to test the SVM, all our model predictions would be wrong as the test and train datasets would be subtely different. As such, our test dataset is rescaled using the rescaling factors from our train dataset. 



If we have a look at our train dataset, we can plot a histogram of the different count of wines in each of the different categories. 

```{r class.source="bg-info", class.output="bg-success", warning = FALSE, message = FALSE}
ggplot(df.train, aes(x=as.numeric(as.character(quality)))) + 
  geom_histogram(color="black", fill="white") + 
  xlab("quality")
```

As we can, see there are many more wines of quality 6, then there are of quality 3 or 8. This may be an issue with our SVM, as it may therefore be inherently biased towards predicting wines of quality 6. As such, we could in theory get a reasonable accuracy for our SVM if it predicted group 6 accurately but all other groups poorly. This would represent a biased model. As such one way we can go about this is using a technique called SMOTE or SCUT to rebalance our dataset to have equal proportions of each of the different wine qualities. SMOTE [(explained more here)](https://towardsdatascience.com/a-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e) is method for generating new data based on existing data in the same group. I'm not going to go into it too much here, but this technique uses the nearest neighbour algorithm to generate new data. SMOTE is the method to use if your data only contains two groups, while SCUT is the method to use if your data comprises three or more groups. Here, we have three different groups so we will be using SCUT. Note that SCUT only needs to be conducted on the train dataset, not the test dataset.

SCUT can be used from the `scutr` package, and run using the following code.


```{r class.source="bg-info", class.output="bg-success"}


df.train <- SCUT(df.train, "quality", oversample="oversample_smote")

```

As such if we replot our dataset, using the following code, we can see that we've now got equal counts of each of the wine qualities in our dataset.

```{r class.source="bg-info", class.output="bg-success", warning = FALSE, message = FALSE}
ggplot(df.train, aes(x=as.numeric(as.character(quality)))) + 
  geom_histogram(color="black", fill="white") + 
  xlab("quality")
```


## Training the SVM

At this point we are now ready to start training our SVM. However, we have to make several decisions about our model. Firstly, we have to decide which kernel to use. For a refresher on kernel choice, see the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm). 

For this analysis, we will be using a radial kernel. 

The next decision we need to make is what values for the cost and gamma parameters we should specify. For a refresher on these parameters, see the [initial explanation of support vector machines (SVMs)](https://benjburgess.github.io/svm). Fortunately, the SVM package we are using (`e1071`) allows us to test various different parameters and then select the best ones.

For the training and testing of the SVM we will be following the advice of [Hsu et al. (2003)](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf). Hsu et al. (2003) advise that SVMs should be generated in two distinct sections. Firstly, a relatively coarse selection of parameter values should be selected which are used to train the model against the training dataset (i.e., training the model). At this point the best parameter values are identified. These parameter values are then used in the next stage (i.e., tuning the model). For tuning the model, a relatively narrow selection of parameter values are then selected which are based on the best parameter values from the model training. This allows for potentially even better model parameters to be identified. 

The range of parameter values to be used is usually at the discretion of the user, although it is usually on a logarithmic scale. Hsu et al. (2003) suggest that values for cost could be $2^{-5}$, $2^{-3}$, ... $2^{15}$ while for gamma the values could be $2^{-15}$, $2^{-13}$, ... $2^{3}$. In the interests of speed, for this analysis we will be using cost parameter values of $2^{-2}$, $2^{-1}$, ... $2^{6}$ and gamma parameter values of $2^{-6}$, $2^{-5}$, ... $2^{2}$.

When training the SVM, the best model parameters are chosen by those which generate an SVM which minimises wines being assigned to the incorrect class (i.e., the model which correctly assigns most wines to the correct class). As part of this procedure, we will be using k-fold cross validation as way of increasing the robustness of our results. I'm not going to go into k-fold cross validation here, but for more information see [this explanation](https://machinelearningmastery.com/k-fold-cross-validation/).

Following this, our SVM can be trained using the following code.

Note that `e1071` package conducts its analysis consecutively, as opposed to parallelly, and can take a while to run (potentially over an hour).

```{r class.source="bg-info", class.output="bg-success"}
svm_trained  <- tune.svm(quality ~ ., #determine wine quality from all other variables
                       data=df.train, #using the df.train dataset
                       tunecontrol = tune.control(cross=10), # use k-fold cross validation with k=10
                       gamma=2^c(seq(-6, 2, by=1)), #select our range of gamma values 
                       cost=2^c(seq(-2, 6, by=1))) #select our range of cost values


summary(svm_trained) #show the summary of our svm training
```

So, as we can see from the model training, the best performing model was when gamma equals `r as.numeric(svm_trained$best.parameters[1])` and cost equals `r as.numeric(svm_trained$best.parameters[2])`. Here the best performance of the model was `r as.numeric(svm_trained$best.performance)` which means that across all 10 folds of the cross validation for these parameter values, ~`r 100*(1-as.numeric(svm_trained$best.performance))`% of all wines were assigned the correct category. This isn't too bad, but there is the potential to further increase this by tuning our SVM. 

However, first we need to extract the best performing model using the following code.


```{r class.source="bg-info", class.output="bg-success"}
svm_trained_model <- svm_trained$best.model
```


## Tuning the SVM

At this stage we have trained the svm and identified the initial parameter values for gamma and cost. In keeping with the recommendations of [Hsu et al. (2003)](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) the next stage of the model development is to tune the model. At this stage, rather than use the coarse gradient of values for each parameter, we use a much more narrow gradient which should allow us to identify better parameter values for our SVM. At this stage for the cost and gamma parameters, we now consider values in the following range which is centerd on the best model parameters from the training stage of the model development: $2^{n - 0.75}$, $2^{n - 0.5}$, $2^{n - 0.25}$, $2^{n}$,... $2^{n+0.75}$. 

The model tuning can be run using the following code.
```{r class.source="bg-info", class.output="bg-success"}
svm_tuned  <- tune.svm(quality ~ .,  #determine wine quality from all other variables
                       data=df.train, #using the df.train dataset
                       tunecontrol = tune.control(cross=10), # use k-fold cross validation with k=10
                       gamma=2^c(seq(log(svm_trained_model$gamma, base=2)-0.75, 
                                     log(svm_trained_model$gamma, base=2)+0.75, by=1/4)), #select our range of gamma values
                        cost=2^c(seq(log(svm_trained_model$cost, base=2)-0.75, 
                                     log(svm_trained_model$cost, base=2)+0.75, by=1/4))) #select our range of cost values

summary(svm_tuned)
```

So, as we can see from our tuning of the SVM, the best performing model has now changed to where gamma equals `r as.numeric(svm_tuned$best.parameters[1])` and cost equals `r as.numeric(svm_tuned$best.parameters[2])`. Compared to the training stage, the tuning of the SVM has not changed the predictive ability of the model, evidenced by the absence of any improvement in the performance of the tuned SVM (`r as.numeric(svm_tuned$best.performance)`). At this stage we'll move from tuning our SVM through to the testing stage.

As before, we need to extract the best performing model using the following code.

```{r class.source="bg-info", class.output="bg-success"}
final_svm_model <- svm_tuned$best.model
```


## Testing the SVM

So as we discussed above, the best forming model is where gamma equals `r as.numeric(svm_tuned$best.parameters[1])` and cost equals `r as.numeric(svm_tuned$best.parameters[2])`. Here across all 10 folds of the cross validation ~`r 100*(1-as.numeric(svm_tuned$best.performance))`% of all wines were assigned to the correct category. This can be seen using the following code.

```{r class.source="bg-info", class.output="bg-success"}
svm_tuned
```

This analysis has so far been based on the training dataset, but at this stage we can now shift our attention to the test dataset we subset from the original data. Using our SVM we can now predict the quality of every wine in the test dataset and compare our predictions to the actual class of the wine. From this we can calculate various different metrics which we can use to assess the performance of our SVM. This can be done using the following code.


```{r class.source="bg-info", class.output="bg-success"}
pred=predict(final_svm_model, df.test)

cm <- caret::confusionMatrix(pred, df.test$quality) 
#this uses the confusionMatrix function from the caret package
cm

```

There's an awful lot of information here, so let's breakdown piece by piece to assess our SVM.

Firstly, let's start with the confusion matrix using the following code.

```{r class.source="bg-info", class.output="bg-success"}

cm$table

```

The confusion matrix is basically a way of allowing us to visually assess whether our SVM assigns our wines (from the test dataset) the correct qualities or, if not, which qualities they are assigned. From our dataset we can see that the SVM broadly classifies wine of qualities 5, 7, or 8 correctly. However, the SVM appears to be less good at classifying wines of qualities 3, 4, and 6. This is just a visual inspection and our initial thoughts can be confirmed (or contrasted) using some statistics.

Let's start by considering the overall statistics for our model, which can be called using the following code.

```{r class.source="bg-info", class.output="bg-success"}

cm$overall

```

So, of the metrics here, the main ones we are going to consider are accuracy and kappa.

If we start with accuracy, the accuracy of our SVM for the test dataset is `r as.numeric(cm$overall[1])*100`% which is lower than the accuracy we obtained from the k-fold cross validation of the SVM at the tuning stage (~`r 100*(1-as.numeric(svm_tuned$best.performance))`%). Overall, this suggests that our SVM may be overfitting our training data. Indeed, the accuracy suggests that our SVM correctly classifies a little over half of all wines correctly. 

The next metric to consider is the kappa value. The kappa value is an alternative measure of accuracy which considers the expected and observed accuracy of our SVM. I'm not going to explain kappa in any great detail (but see [this comprehensive explanation on stackoverflow](https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english)), but as with the accuracy metric the higher the value of kappa the better. Kappa generally reflects a less biased measure of the accuracy of a model. As such, the value of kappa here indicates that our SVM does not do a great job of classifying wine quality correctly.

Overall, these metrics are two of the most commonly used metrics for assessing the performance of an SVM. 

The final thing we can do when assessing our SVM performance is to consider how the performance varies between different wine qualities (much as we visually did for the confusion matrix above). This can be done by running the following code. 

```{r class.source="bg-info", class.output="bg-success"}

cm$byClass

```

As we can see we have a variety of different metrics for each of the different wine qualities. Here, we are only going to focus on the final column of balanced accuracy, but [an explanation of the other metrics can be found here](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124?gi=f0ce52bb598).

We can cut out the other metrics, and focus on balanced accuracy using the following code.

```{r class.source="bg-info", class.output="bg-success"}

cm$byClass[,11]*100

```

While the variable name is now missing, by comparing to the above results we can confirm that this is indeed balanced accuracy which has simply been converted to a percentage.

As you can see, for wines of qualities 5, 7, and 8, the balanced accuracy is as high as `r max(as.numeric(cm$byClass[,11]*100))`%. Much as we expected from our expectation of the confusion matrix, our SVM does a reasonable job of classifying wines of quality 5, 7, and 8, but a much poorer job of correctly classifying wines of quality 3, 4, or 6. This is really important to understand as while our overall model accuracy is `r as.numeric(cm$overall[1])*100`% this actually varies between the different wine qualities. This might be really important for our analyses, as if our primary aim was to build a machine learning model which is capable of accurately determining when a wine is of a very poor quality (i.e., qualities 3 or 4) then the SVM does a relatively poor job of this, suggesting that we may need to adjust our analysis. However, if our aim was to build a model which is capable of accurately determining wines of a higher quality (i.e., qualities 7 or 8) then our analysis may suffice as it appears to do reasonably well at accurately classifying wines of these qualities.

## Improving the SVM

As we've shown from the model testing we have developed an SVM which does not do as a good job of classifying wines into different qualities as we would perhaps have hoped. As such, we may wish to further improve the model. On such approach to achieve this may be to further tune the SVM with an even narrower range of parameter values for cost and gamma, however we need to ensure that we are not going to overfit the model when doing this. If our aim was to simply identify whether a wine was of a 'good' or 'bad' quality, then we may wish to recode our data by classifying wines with a quality of 6 or higher as 'good' and 5 or lower as 'bad'. Rerunning the SVM with this recoded data may result in a SVM with a higher predictive ability. Another alternative choice we could make may be to only consider either red or white wines as opposed to building an SVM which attempts to accurately classify both. An additional option we might wish to explore is the use of dimensionality reduction approaches (e.g., Principal Component Analysis) on our chemical data, the use of a few core principal components may help us to remove some of the 'noise' in our data and result in a more accurate model. Overall, any attempts to improve the model are likely to represent a trade-off between many different choices. Indeed, it may be that our relatively limited dataset (~6000 data points) is so inherently noisy (i.e., absence of any clear boundaries between the different wine qualities) that generating an effective SVM capable of classifying wine qualities with a high degree of accuracy may not necessarily be possible.


## Concluding remarks

In this tutorial, we have build a support vector machine (SVM) which may not be as effective at classifying wines of different qualities as we would of initially hoped. However, this tutorial outlines the various different data cleaning and analysis stages which are essential for the generation of an SVM. Importantly, this tutorial illustrates the potential limitations of SVMs highlighting that they are not necessarily an appropriate approach for some datasets, although potential options for improving our SVM are also discussed. 


## References

Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Modeling wine preferences by data mining from physicochemical properties. *Decision Support Systems*, 47(4), 547-553.

Hsu, C. W., Chang, C. C., & Lin, C. J. (2003). *A practical guide to support vector classification*.


## Links

[benjburgess.github.io](https://benjburgess.github.io/)

[Support Vector Machines](https://benjburgess.github.io/data/index/svm)

